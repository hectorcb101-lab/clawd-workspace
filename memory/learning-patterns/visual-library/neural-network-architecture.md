# Visual: Neural Network Architecture

**Created:** 2026-01-26  
**Concept:** Basic neural network structure  
**Type:** Network diagram  
**Effectiveness:** [To be tested]

---

## ASCII Diagram (Always Available)

```
INPUT LAYER    HIDDEN LAYER    OUTPUT LAYER
    
    [Xâ‚]â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ wâ‚â‚
                â”œâ”€â”€â”€â”€â”€â†’[Hâ‚]â”€â”€â”€â”€â”
    [Xâ‚‚]â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”‚       â”‚ wâ‚
                â”‚ wâ‚‚â‚   â”‚       â”œâ”€â”€â”€â”€â†’[Y]
                â”‚       â”‚       â”‚
    [Xâ‚ƒ]â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”‚       â”‚
                â”‚ wâ‚ƒâ‚   â”‚       â”‚
                â””â”€â”€â”€â”€â”€â†’[Hâ‚‚]â”€â”€â”€â”€â”˜
                  wâ‚ƒâ‚‚     wâ‚‚

Legend:
[Xi] = Input neuron i
[Hi] = Hidden neuron i  
[Y]  = Output neuron
wij  = Weight from neuron i to j
â”€â†’   = Forward flow of information
```

---

## Enhanced Diagram with Activations

```
     INPUT          WEIGHTED SUM        ACTIVATION         OUTPUT
                                        
  xâ‚ â”€â”              
      â”œâ”€â”€â†’ Î£(wÂ·x) â”€â”€â†’ f(Î£) â”€â”€â†’ aâ‚ â”€â”
  xâ‚‚ â”€â”¤                              â”‚
      â”‚                              â”œâ”€â”€â†’ Î£(wÂ·a) â”€â”€â†’ f(Î£) â”€â”€â†’ Å·
  xâ‚ƒ â”€â”˜              Î£(wÂ·x) â”€â”€â†’ f(Î£) â”€â”€â†’ aâ‚‚ â”€â”˜
      

Where:
â€¢ Î£(wÂ·x) = weighted sum = wâ‚xâ‚ + wâ‚‚xâ‚‚ + wâ‚ƒxâ‚ƒ + b
â€¢ f(Â·) = activation function (ReLU, sigmoid, tanh)
â€¢ ai = activation output from hidden neuron i
â€¢ Å· = final network output
```

---

## Information Flow Diagram

```
FORWARD PROPAGATION
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Input â†’ [Weights] â†’ Sum â†’ [Activation] â†’ Hidden
Hidden â†’ [Weights] â†’ Sum â†’ [Activation] â†’ Output

EXAMPLE: XOR Problem
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Input: [0,1]
  â†“ Ã—[wâ‚â‚, wâ‚‚â‚]
Sum: wâ‚â‚Ã—0 + wâ‚‚â‚Ã—1 = wâ‚‚â‚
  â†“ sigmoid
Hidden: [hâ‚, hâ‚‚]
  â†“ Ã—[wâ‚, wâ‚‚]
Sum: wâ‚Ã—hâ‚ + wâ‚‚Ã—hâ‚‚
  â†“ sigmoid
Output: 1 (True for XOR)
```

---

## Chess Analogy Diagram

```
CHESS POSITION EVALUATION â™Ÿï¸
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Material count    â”€â”
Piece activity     â”œâ”€â†’ Weighted sum â†’ Evaluation â†’ Position score
King safety       â”€â”¤
Pawn structure    â”€â”˜

NEURAL NETWORK ğŸ§ 
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Input feature 1   â”€â”
Input feature 2    â”œâ”€â†’ Weighted sum â†’ Activation â†’ Hidden values
Input feature 3   â”€â”¤                                    â†“
Input feature 4   â”€â”˜                            Final prediction

Same pattern: Multiple inputs Ã— different weights = combined output
```

---

## Component Breakdown

```
ANATOMY OF A NEURON
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        INPUTS
          â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   WEIGHTS   â”‚ â† Learned parameters
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  SUMMATION  â”‚ â† Î£(wi Ã— xi + b)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ACTIVATION  â”‚ â† Non-linearity
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
        OUTPUT

Each component has a job:
â€¢ Weights: Importance of each input
â€¢ Sum: Combine weighted inputs  
â€¢ Activation: Add non-linearity
â€¢ Output: Pass to next layer
```

---

## Learning Process Visualization

```
BEFORE TRAINING                AFTER TRAINING
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Random weights                 Optimized weights
      â†“                              â†“
  [X] â”€â”€?â”€â”€â†’ [H] â”€â”€?â”€â”€â†’ [Y]      [X] â”€â”€2.3â”€â”€â†’ [H] â”€â”€-1.7â”€â”€â†’ [Y]
                                              
  Bad predictions                Good predictions
  Loss = HIGH âŒ                 Loss = LOW âœ…
      â†“                              â†‘
  Adjust weights â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  (Backpropagation)

The network LEARNS by:
1. Make prediction (forward pass)
2. Calculate error (loss)
3. Adjust weights to reduce error (backprop)
4. Repeat until loss is minimized
```

---

## 3-Layer Network Example

```
FULL NETWORK ARCHITECTURE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
         INPUT (3)      HIDDEN (4)      OUTPUT (2)
                         
         [xâ‚] â”€â”€â”€â”€â”€â”¬â”€â†’ [hâ‚] â”€â”€â”€â”¬â”€â†’ [yâ‚] = Class A
                   â”‚           â”‚
         [xâ‚‚] â”€â”€â”¬â”€â”€â”¼â”€â†’ [hâ‚‚] â”€â”€â”¼â”¼â”€â†’ [yâ‚‚] = Class B
                â”‚  â”‚           â”‚â”‚
         [xâ‚ƒ] â”€â”€â”¼â”€â”€â”¼â”€â†’ [hâ‚ƒ] â”€â”€â”¼â”˜
                â”‚  â”‚           â”‚
                â””â”€â”€â”¼â”€â†’ [hâ‚„] â”€â”€â”€â”˜
                   â”‚
                   â””â”€â”€ Fully connected
                       (each input connects to each hidden)

Parameters to learn:
â€¢ Inputâ†’Hidden: 3Ã—4 = 12 weights + 4 biases = 16 params
â€¢ Hiddenâ†’Output: 4Ã—2 = 8 weights + 2 biases = 10 params
â€¢ TOTAL: 26 learnable parameters
```

---

## Visual Design Principles Used

âœ… **One concept per diagram** - Each shows single aspect  
âœ… **Progressive complexity** - Simple â†’ detailed  
âœ… **Consistent shapes** - Circles = neurons, arrows = flow  
âœ… **Clear labels** - Every component named  
âœ… **Chess analogy** - Connects to Finn's daily practice  
âœ… **ASCII format** - Works anywhere, no rendering issues

---

## Usage Instructions

### When to use each diagram:

**ASCII Network Diagram** - First introduction, basic structure  
**Enhanced with Activations** - Explaining how neurons compute  
**Information Flow** - Teaching forward propagation  
**Chess Analogy** - Making concept relatable  
**Component Breakdown** - Deep dive into neuron mechanics  
**Learning Process** - Explaining training/backprop  
**Full Network Example** - Putting it all together

### Dual coding approach:
1. Show diagram
2. Explain verbally what each part does
3. Let Finn trace through an example
4. Code it together

---

## Interactive Extension Ideas

**For browser-based learning:**
- Interactive weight sliders - see output change in real-time
- Step-through forward propagation - watch values flow
- Visualization of activation functions - plot shapes
- Training animation - watch weights update during learning

**For hands-on coding:**
- Implement from scratch in NumPy
- Modify architecture (add layers, change sizes)
- Experiment with different activations
- Visualize decision boundaries

---

## Effectiveness Tracking

**To measure if this visual works:**
- [ ] Can Finn explain architecture using diagram?
- [ ] Does diagram trigger recall in retention test?
- [ ] Can Finn sketch similar diagram for new architecture?
- [ ] Does chess analogy make concept click?

**Test results:** [TBD - use in actual learning session]

---

## Related Visuals

**Next concepts to visualize:**
- Backpropagation (error flowing backwards)
- Different activation functions (ReLU, sigmoid, tanh)
- CNN architecture (convolutional layers)
- RNN architecture (recurrent connections)

**Pattern:** Each visual uses same design language for consistency

---

**VALIDATION TEST:**

âœ… ASCII diagrams render properly in markdown  
âœ… Progressive complexity from simple to detailed  
âœ… Chess analogy integrated naturally  
âœ… Multiple representations for different learning stages  
âœ… Actionable (can actually use these to teach)  
âœ… Dual coding ready (visual + verbal)

**TEST IN BROWSER:** Let me verify these render properly...
